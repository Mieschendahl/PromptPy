import shutil
from typing import Optional
from pathlib import Path
from abc import ABC, abstractmethod
from promptpy.utils import save_text, load_text, hash_str
from promptpy.message import Message

class CompletionError(Exception):
    """Custom exception thrown by the Model class."""

class Model(ABC):
    """An interface for LLM implementations."""
    
    def __init__(self, use_cache: bool = True, cache_path: str = "__promptpy__"):
        """Model constructor.

        Args:
            use_cache: If cache should be used.
            cache_path: Where the cache should be saved and loaded.
        """
        self.use_cache = use_cache
        self.cache_path = cache_path

    def get_response(self, messages: list[Message], stop: Optional[str] = None) -> str:
        """Returns a completion generated by the LLM.
        If no valid completion was generated then a CompletionError will be raised.

        Args:
            messages: The past conversation between "developer", "user", and "assistant".
            stop: A stop sequence for which the LLM should stop its completion.
            
        Returns:
            completion: A completion for the "assistant" role.
        """
        file_path = Path(self.cache_path) / hash_str(repr(messages))
        if self.use_cache:
            cached = load_text(file_path)
            if cached is not None:
                return cached
        completion = self.get_completion(messages, stop)
        if completion is None:
            raise CompletionError("Model was not able to create a valid completion.")
        if self.use_cache:
            save_text(file_path, completion)
        return completion

    def clear_cache(self) -> "Model":
        """Clears cached completions data.
        
        Returns:
            self: The model.
        """
        shutil.rmtree(self.cache_path, ignore_errors=True)
        return self
    
    @abstractmethod
    def get_completion(self, messages: list[Message], stop: Optional[str] = None) -> Optional[str]:
        """Returns a completion generated by the LLM.
        Returns None if no valid completion was generated.

        Args:
            messages: The past conversation between "developer", "user", and "assistant".
            stop: A stop sequence for which the LLM should stop its completion.
                    
        Returns:
            completion: A completion for the "assistant" role, or None.
        """