import shutil
from typing import Optional, Self
from pathlib import Path
from abc import ABC, abstractmethod
from promptpy.utils import save_text, load_text, hash_str
from promptpy.message import Message

class CompletionError(Exception):
    """Custom exception thrown by the Model class."""

class Model(ABC):
    """An interface for LLM implementations."""
    
    def __init__(self: Self):
        """Model constructor."""
        self.set_cache(None)
    
    def set_cache(self, cache_path: Optional[str] = None) -> Self:
        """Sets the caching configuration.
        
        Args:
            cache_path: Where the cache should be saved and loaded.
                If None, then no caching is used.

        Returns:
            self: The model.
        """
        self.cache_path = cache_path
        return self

    def get_response(self, messages: list[Message], stop: Optional[str] = None) -> str:
        """Returns a completion generated by the LLM.
        If no valid completion was generated then a CompletionError will be raised.

        Args:
            messages: The past conversation between "developer", "user", and "assistant".
            stop: A stop sequence for which the LLM should stop its completion.
            
        Returns:
            completion: A completion for the "assistant" role.
        """
        if self.cache_path is not None:
            file_path = Path(self.cache_path) / hash_str(repr(messages))           
            cached = load_text(file_path)
            if cached is not None:
                return cached
        completion = self.get_completion(messages, stop)
        if completion is None:
            raise CompletionError("Model was not able to create a valid completion.")
        if self.cache_path is not None:
            save_text(file_path, completion)  # type: ignore
        return completion

    def clear_cache(self, cache_path: Optional[str] = None) -> Self:
        """Clears cached completions data.
        
        Args:
            cache_path: The path to the cache.
                If None, then self.cache_path is used.
        
        Returns:
            self: The model.
        """
        cache_path = self.cache_path if cache_path is None else cache_path
        if cache_path is not None:
            shutil.rmtree(cache_path, ignore_errors=True)
        return self
    
    @abstractmethod
    def get_completion(self, messages: list[Message], stop: Optional[str] = None) -> Optional[str]:
        """Returns a completion generated by the LLM.
        Returns None if no valid completion was generated.

        Args:
            messages: The past conversation between "developer", "user", and "assistant".
            stop: A stop sequence for which the LLM should stop its completion.
                    
        Returns:
            completion: A completion for the "assistant" role, or None.
        """