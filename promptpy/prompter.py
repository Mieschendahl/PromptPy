from typing import Optional, TextIO
from promptpy.utils import random_str, pad
from promptpy.message import Message
from promptpy.model import Model
from promptpy.option import stop, seperator, Option

class ChoiceError(Exception):
    """Costum exception for the Prompter class."""

class Prompter:
    """A class for interacting with an LLM."""
    _last_id: Optional[str] = None

    def __init__(self, model: Model, messages: Optional[list[Message]] = None, log_file: Optional[TextIO] = None, allow_injections: bool = False, id: Optional[str] = None):
        """Prompter constructor.
        
        Args:
            model: The model to be used for generating completions.
            messages: Messages from a previous conversation.
            log_file: The file where to save the logs to, or None to not generate logs.
            allow_injections: Whether the prompter prompts the user for a message before letting the model generate a completion.
            id: The id for the Prompter, which  will be used for in the logs for readability.
        """
        self.model = model
        self.messages = [] if messages is None else messages
        self.log_file = log_file
        self.allow_injections = allow_injections
        self.id = random_str() if id is None else id
    
    def copy(self) -> "Prompter":
        """Copies the Prompter."""
        return Prompter(self.model, self.messages, self.log_file, self.allow_injections, self.id)

    def log_message(self, *messages: Message) -> "Prompter":
        """Logs the given messages to the log file.
        
        Args:
            messages: The messages that should be logged.
            
        Returns:
            self: The Prompter.
        """
        if self.log_file is None:
            return self
        logs = []
        if Prompter._last_id != self.id:  # Visually marks the beginning of the prompter
            Prompter._last_id = self.id
            logs.append(f"PROMPTER \"{self.id}\"\n\n")
        for message in messages:
            logs.extend([
                message.role.upper() + ":\n",
                pad(message.content, padding=" | ") + "\n\n"
            ])
        print(*logs, sep="", end="", file=self.log_file)
        return self

    def _add_message(self, *messages: Message) -> "Prompter":
        """Adds messages to the conversation.

        Args:
            messages: The messages that should be added.
            
        Returns:
            self: The Prompter.
        """
        for message in messages:
            self.messages.append(message)
            self.log_message(self.messages[-1])
        return self
    
    def add_message(self, *messages: str, role="user") -> "Prompter":
        """Adds messages to the conversation.

        Args:
            messages: The contents of messages that should be added.
            role: The role that the messages should have.
                
        Returns:
            self: The Prompter.
        """
        return self._add_message(*[Message(message, role) for message in messages])

    def add_response(self, stop: Optional[str] = None) -> "Prompter":
        """Adds a response from the LLM, based on the current messages.
        The user can manually inject a prompt before the LLM response is generated, if allow_injections is set to true.

        Args:
            stop: Stop sequence for which the LLM should stop the response.
            
        Returns:
            self: The Prompter.
        """
        if self.allow_injections:
            prompt = input("INJECTION (â†µ: continue, x: exit): ")
            print()
            if prompt == "x":
                exit(0)
            if prompt != "":
                self.add_message(prompt)
        response = self.model.get_response(self.messages, stop).strip()
        self.add_message(response, role="assistant")
        return self

    def get_response(self, stop: Optional[str] = None) -> str:
        """Adds a response from the LLM, based on the current messages.
        The user can manually inject a prompt before the LLM response is generated, if allow_injections is set to true.

        Args:
            stop: Stop sequence for which the LLM should stop the response.
            
        Returns:
            response: The completion generated by the model.
        """
        self.add_response(stop=stop)
        return self.messages[-1].content

    def get_choice(self, *options: Option, role="developer") -> tuple[str, str]:
        """Gets a choice from an LLM and returns the label of the option that was chosen as well as any
        data which the LLM was instructed to generate for this option.
        
        Args:
            options: A list of options that the LLM can choose from.
            role: The role that the choice instructions should have.
        
        Returns:
            (label, data):
                label: The label of the chosen option.
                data: The data specifically generated for this option.
        """
        description = "\n\n".join(option.describe() for option in options)
        self.add_message(f"Use one of the following options:\n\n{description}", role=role)
        response = self.get_response(stop=stop)
        if seperator not in response:
            raise ChoiceError(f"The model did not adhere to the selection format: \"{response}\"")
        label, data = map(str.strip, response.split(seperator, 1))
        if label not in {option.label for option in options}:
            raise ChoiceError(f"The model made an invalid choice: \"{label}\"")
        return label, data